
# Logistic回归

## 前言

### 什么是回归？

* 假设有一些数据点，我们用一条直线对这些点进行拟合（该线称为最佳拟合直线），这个拟合过程就称作回归。

### 利用Logistics回归进行分类的主要思想是什么？

* 根据现有数据对分类边界线建立回归公式，以此进行分类。
* 这里的“回归”一词源于最佳拟合，表示要找到最佳拟合参数集。
* 训练分类器时的做法是寻找最佳拟合参数，使用的是最优化算法。

### 二值型输出分类器的数学原理（Logistic回归的一般过程）：

1. 收集数据：采用任意方法收集数据
2. 准备数据：由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳。
3. 分析数据：采用任意方法对数据进行分析。
4. 训练算法：大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数。
5. 测试算法：一旦训练步骤完成，分类将会很快。
6. 使用算法：首先，我们需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定他们属于哪个类别；在这之后，我们就可以在输出的类别上做一些其他分析工作。

## 基于Logistic 回归和 Sigmoid 函数的分类

### Logistic 回归的优缺点

* 优点：计算代价不高，易于理解和实现
* 缺点：容易欠拟合，分类精度可能不高
* 适用数据类型：数值型和标称型数据

### Sigmoid函数

* 性质：函数输出0或者1，是一种阶跃函数（step function）。在数学上，如果实数域上的某个函数可以用半开区间上的指示函数的有限次线性组合来表示，那么个函数就是阶跃函数。而数学中指示函数（indicator function）是定义在某集合X上的函数，表示其中有哪些元素属于某一子集A。

* 最佳回归系数：weight,或者叫系数

## 基于最优化方法的最佳回归系数确定

### 最优化方法一：梯度上升

* 思想：要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。
* 梯度上升算法到达每个点后都会重新估计移动的方向：从P0开始，计算完该点的梯度，函数就根据梯度移动到下一点P1。在P1点，梯度再次被重新计算，并沿新的梯度方向移动到P2。如此循环迭代，直到满足停止条件。迭代的过程中，梯度算子总是保证我们能选取到最佳的移动方向。
* 梯度算子总是指向函数值增长最快的方向。

**梯度下降算法**

* 梯度下降算法与梯度上升算法一样，只是公式中的加法需要变成减法。
* 梯度上升算法用来求函数的最大值，梯度下降算法用来求函数的最小值。

### 训练算法：使用梯度上升找到最佳参数

### 分析数据：画出决策边界

### 训练算法：随机梯度上升

**一次仅用一个样本点来更新回归系数，该方法称为随机梯度上升算法**

由于可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升算法是一个在线学习算法。与“在线学习”相对应，一次处理所有数据被称为“批处理”。

随机梯度上升算法与梯度上升算法效果相当，但占用更少的计算资源。
